(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[969],{743:function(A,e,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/GPT",function(){return s(8934)}])},9770:function(A,e){"use strict";e.Z={src:"/_next/static/media/gpt.ef1e2e4b.jpg",height:730,width:1200,blurDataURL:"data:image/jpeg;base64,/9j/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wgARCAAFAAgDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAP/xAAVAQEBAAAAAAAAAAAAAAAAAAACBf/aAAwDAQACEAMQAAABmFF//8QAFRABAQAAAAAAAAAAAAAAAAAAESH/2gAIAQEAAQUCZ//EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAIAQMBAT8Bf//EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAIAQIBAT8Bf//EABUQAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEBAAY/Ao//xAAYEAACAwAAAAAAAAAAAAAAAAAAUQERQf/aAAgBAQABPyFWHen/2gAMAwEAAgADAAAAEAv/xAAVEQEBAAAAAAAAAAAAAAAAAAAAAf/aAAgBAwEBPxCv/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPxB//8QAGRAAAQUAAAAAAAAAAAAAAAAAEQAxQVGR/9oACAEBAAE/EDhyzg0v/9k=",blurWidth:8,blurHeight:5}},8934:function(A,e,s){"use strict";s.r(e);var a=s(5893),r=s(5675),t=s.n(r);s(7294);var l=s(9770),n=s(9352),c=s(1664),i=s.n(c);e.default=()=>(0,a.jsxs)("div",{className:"w-full",children:[(0,a.jsxs)("div",{className:"w-screen h-[50vh] relative",children:[(0,a.jsx)("div",{className:"absolute top-0 left-0 w-full h-[50vh] bg-black/70 z-10"}),(0,a.jsx)(t(),{className:"absolute z-1",layout:"fill",objectFit:"cover",src:l.Z,alt:"/"}),(0,a.jsxs)("div",{className:"absolute top-[70%] max-w-[1240px] w-full left-[50%] right-[50%] translate-x-[-50%] translate-y-[-50%] text-white z-10 p-2",children:[(0,a.jsx)("h2",{className:"py-2",children:"Decoder Transformer (GPT)"}),(0,a.jsx)("h3",{children:"Transformer / PyTorch / HuggingFace"})]})]}),(0,a.jsxs)("div",{className:"max-w-[1240px] mx-auto p-2 grid md:grid-cols-5 gap-8 py-8",children:[(0,a.jsxs)("div",{className:"col-span-4",children:[(0,a.jsx)("p",{children:"Project"}),(0,a.jsx)("h2",{className:"py-6",children:"Overview"}),(0,a.jsx)("p",{className:"text-justify text-xl",children:"This project involved developing a decoder transformer or GPT (Generative Pre-training Transformer) model with several causal self-attention mechanisms for natural language understanding (NLU) tasks. I implemented the transformer architecture from scratch using PyTorch and trained on the GLUE (General Language Understanding Evaluation) dataset from the Hugging Face library."}),(0,a.jsx)("a",{href:"https://github.com/hamednasr/transformers/blob/main/Decoder_transformers_in_pytorch.ipynb",target:"_blank",rel:"noreferrer",children:(0,a.jsx)("button",{className:"px-8 py-2 mt-4 mr-8",children:"Code"})}),(0,a.jsx)("a",{target:"_blank",rel:"noreferrer",children:(0,a.jsx)("button",{className:"px-8 py-2 mt-4",children:"Demo"})})]}),(0,a.jsx)("div",{className:"col-span-4 md:col-span-1 shadow-xl shadow-gray-400 rounded-xl py-4",children:(0,a.jsxs)("div",{className:"p-2",children:[(0,a.jsx)("p",{className:"text-center font-bold pb-2",children:"Technologies"}),(0,a.jsxs)("div",{className:"grid grid-cols-3 md:grid-cols-1",children:[(0,a.jsxs)("p",{className:"text-gray-600 py-2 flex items-center",children:[(0,a.jsx)(n.Svj,{className:"pr-1"})," Transformer"]}),(0,a.jsxs)("p",{className:"text-gray-600 py-2 flex items-center",children:[(0,a.jsx)(n.Svj,{className:"pr-1"})," PyTorch"]}),(0,a.jsxs)("p",{className:"text-gray-600 py-2 flex items-center",children:[(0,a.jsx)(n.Svj,{className:"pr-1"})," HuggingFace"]}),(0,a.jsxs)("p",{className:"text-gray-600 py-2 flex items-center",children:[(0,a.jsx)(n.Svj,{className:"pr-1"})," Numpy"]}),(0,a.jsxs)("p",{className:"text-gray-600 py-2 flex items-center",children:[(0,a.jsx)(n.Svj,{className:"pr-1"})," Self Attention"]})]})]})}),(0,a.jsx)(i(),{href:"/#projects",children:(0,a.jsx)("p",{className:"underline cursor-pointer",children:"Back"})})]})]})}},function(A){A.O(0,[937,774,888,179],function(){return A(A.s=743)}),_N_E=A.O()}]);